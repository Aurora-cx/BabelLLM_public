import openai
import json
import time
from pathlib import Path
from transformers import AutoTokenizer
from collections import deque
import re

# åŠ è½½ Qwen Tokenizerï¼ˆæˆ–æ¢æˆä½ çš„æ¨¡å‹ï¼‰
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-1.8B-Chat")

# è®¾ç½®ä½ çš„ OpenAI API Key
client = openai.OpenAI(api_key="")
def extract_json(text):
    # åˆ é™¤ markdown ä»£ç å—æ ¼å¼
    if text.strip().startswith("```json"):
        return re.sub(r"^```json\\n?|\\n?```$", "", text.strip(), flags=re.MULTILINE).strip()
    return text.strip()
# 8 ä¸ªé¢†åŸŸ
zh_domains = [
    "å®¶åº­æ—¥å¸¸", 
    # "è‡ªç„¶ç°è±¡", "æ ¡å›­ç”Ÿæ´»", "åŒ»ç–—å¥åº·",
    # "å•†ä¸šæ¶ˆè´¹", "èŒåœºåŠå…¬", "å…¬å…±äº¤é€š", "å¨±ä¹ä¼‘é—²"
]

domain_mapping = {
    "å®¶åº­æ—¥å¸¸": "household_routine",
    "è‡ªç„¶ç°è±¡": "natural_events",
    "æ ¡å›­ç”Ÿæ´»": "school_life",
    "åŒ»ç–—å¥åº·": "healthcare",
    "å•†ä¸šæ¶ˆè´¹": "shopping_retail",
    "èŒåœºåŠå…¬": "workplace_activities",
    "å…¬å…±äº¤é€š": "public_transportation",
    "å¨±ä¹ä¼‘é—²": "leisure_recreation"
}

# è¾“å‡ºæ–‡ä»¶å¤¹
output_dir = Path("data/clean_causal_data_v2")
output_dir.mkdir(parents=True, exist_ok=True)

# Promptæ¨¡æ¿
def generate_prompt(domain, used_subjects_en, used_verbs_en, used_subjects_zh, used_verbs_zh):
    banned_subjects_en = ", ".join(used_subjects_en) if used_subjects_en else "(none)"
    banned_verbs_en = ", ".join(used_verbs_en) if used_verbs_en else "(none)"
    banned_subjects_zh = "ã€".join(used_subjects_zh) if used_subjects_zh else "ï¼ˆæ— ï¼‰"
    banned_verbs_zh = "ã€".join(used_verbs_zh) if used_verbs_zh else "ï¼ˆæ— ï¼‰"
    
    base_prompt = f"""Please generate a 3-step causal chain in the field of {domain}.

You must follow all of these constraints:

1. The English sentence must follow this structure:  
   "Once [subject1][verb1], [subject2][verb2], then [subject3][verb3]."  
   - Each subject and each verb must be a single word.  
   - Select verbs that can form complete meaningful sentences without requiring an object.
   - Do not include determiners, adjectives, or multi-word phrases.  
   - Make sure every component (subject1, verb1, subject2, verb2, subject3, verb3) can be tokenized into exactly one token by Qwen-1.5-1.8B-Chat.
   - Example (correct): "Once temperature rises, ice melts, then water flows."  
   - Example (incorrect): "Once the temperature is rising..."

2. The Chinese sentence must mirror the same structure:  
   "ä¸€æ—¦[subject1][verb1]ï¼Œ[subject2][verb2]ï¼Œç„¶å[subject3][verb3]ã€‚"  
   - Each subject and verb must be a single standalone Chinese word, with no modifiers or compound forms.  
   - Example (correct): "æ¸©åº¦ä¸€æ—¦å‡é«˜ï¼Œå†°å°±èåŒ–ï¼Œç„¶åæ°´å°±æµåŠ¨ã€‚"  
   - Example (incorrect): "æ¸©åº¦ä¸€æ—¦è¢«å‡é«˜..."

3. Also generate a question-answer pair for both languages:
   - English question: "Therefore, if [cause_subject] [cause_verb], the final result is"
   - English answer: "[final_subject] [final_verb]."
   - Chinese question: "å› æ­¤ï¼Œå¦‚æœ[cause_subject][cause_verb]ï¼Œæœ€ç»ˆç»“æœæ˜¯"
   - Chinese answer: "[final_subject][final_verb]ã€‚"
"""
    example_prompt = """
Return your result in the following JSON format:

{
  "en": [
    "Once temperature rises, ice melts, then water flows.",
    "Therefore, if temperature rises, the final result is"
  ],
  "zh": [
    "æ¸©åº¦ä¸€æ—¦å‡é«˜ï¼Œå†°å°±èåŒ–ï¼Œç„¶åæ°´å°±æµåŠ¨ã€‚",
    "å› æ­¤ï¼Œå¦‚æœæ¸©åº¦å‡é«˜ï¼Œæœ€ç»ˆç»“æœæ˜¯"
  ],
  "answer": {
    "en": "water flows.",
    "zh": "æ°´æµåŠ¨ã€‚"
  },
  "labels": {
    "en": {
      "cause_subject": "temperature",
      "cause_verb": "rises",
      "intermediate_subject": "ice",
      "intermediate_verb": "melts",
      "final_subject": "water",
      "final_verb": "flows"
    },
    "zh": {
      "cause_subject": "æ¸©åº¦",
      "cause_verb": "å‡é«˜",
      "intermediate_subject": "å†°",
      "intermediate_verb": "èåŒ–",
      "final_subject": "æ°´",
      "final_verb": "æµåŠ¨"
    }
  },
  "causal_labels":{
    "en":{
      "cause_sentence": "Once temperature rises,",
      "effect_sentence_1": "ice melts,",
      "effect_sentence_2": "then water flows."
    },
    "zh":{
      "cause_sentence": "æ¸©åº¦ä¸€æ—¦å‡é«˜ï¼Œ",
      "effect_sentence_1": "å†°å°±èåŒ–ï¼Œ",
      "effect_sentence_2": "ç„¶åæ°´å°±æµåŠ¨ã€‚"
    }
  }
}"""


    banned_words = f"""\DO NOT use these words in the causal chain:
EnglishSubjects: {banned_subjects_en}
ChineseSubjects: {banned_subjects_zh}
"""

    final_instructions = f"""
âš  Important: 
- All subject and verb entries must be single words (1 word only).   
- Do not use multi-word phrases like "the button" or "turns off".  
- All values in the "labels" must be a single word.
- Only generate literal, real-world events. 
- Check if the content cling to the {domain}.
- Check if the cause-effect relationship is natural and reasonable.
- Only return valid JSON string that can be parsed by Python directly.
"""

    return base_prompt + banned_words+ example_prompt  + final_instructions

def label_token_lengths(labels, lang="en"):
    return {k: len(tokenizer.tokenize(v)) for k, v in labels[lang].items()}

def is_valid_labels(labels, lang="en"):
    for key, val in labels[lang].items():
        # æ–°ç­–ç•¥ï¼šå…è®¸å¤š tokenï¼Œåªæ‹’ç»å¸¦ç©ºæ ¼æˆ–æ˜æ˜¾çŸ­è¯­çš„
        if " " in val.strip() or len(val.strip()) == 0:
            return False
    return True

# ä¸ºæ¯ä¸ªé¢†åŸŸç”Ÿæˆ50æ¡å¹²å‡€æ•°æ®ï¼ŒåŠ å…¥ç†”æ–­æœºåˆ¶ï¼ˆæœ€å¤šå°è¯•100æ¬¡ï¼‰
def generate_clean_data(domain, n_target=50, max_trials=100):
    clean_samples = []
    tries = 0
    used_subjects_en = deque(maxlen=60)
    used_verbs_en = deque(maxlen=60)
    used_subjects_zh = deque(maxlen=60)
    used_verbs_zh = deque(maxlen=60)
    print(f"\nğŸš€ Generating samples for domain: {domain}")
    
    while len(clean_samples) < n_target and tries < max_trials:
        tries += 1
        try:
            prompt = generate_prompt(domain, used_subjects_en, used_verbs_en, used_subjects_zh, used_verbs_zh)
            print(f"\nTrial {tries}: Sending prompt to GPT-4...")
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.9,
            )
            text = response.choices[0].message.content
            print(f"Received response: {text[:200]}...")  # åªæ‰“å°å‰200ä¸ªå­—ç¬¦
            text = extract_json(text)
            parsed = json.loads(text)
            print(parsed)
            
            if "labels" not in parsed:
                print(f"â›” Missing 'labels' field at trial {tries}")
                continue
            
            if is_valid_labels(parsed["labels"], "en") and is_valid_labels(parsed["labels"], "zh"):
                parsed["token_lengths"] = {
                    "en": label_token_lengths(parsed["labels"], "en"),
                    "zh": label_token_lengths(parsed["labels"], "zh")
                }
                if parsed["labels"]['en']['cause_subject'] not in used_subjects_en:
                    clean_samples.append(parsed)
                else:
                    print(f"âŒ [{len(clean_samples)}/{n_target}] {parsed['en'][0]}")  # åªæ‰“å°è‹±æ–‡å¥å­
                print(f"âœ… [{len(clean_samples)}/{n_target}] {parsed['en'][0]}")  # åªæ‰“å°è‹±æ–‡å¥å­
                used_subjects_en.append(parsed["labels"]["en"]["cause_subject"])
                used_subjects_en.append(parsed["labels"]["en"]["intermediate_subject"])
                used_subjects_en.append(parsed["labels"]["en"]["final_subject"])
                used_verbs_en.append(parsed["labels"]["en"]["cause_verb"])
                used_verbs_en.append(parsed["labels"]["en"]["intermediate_verb"])
                used_verbs_en.append(parsed["labels"]["en"]["final_verb"])
                used_subjects_zh.append(parsed["labels"]["zh"]["cause_subject"])
                used_subjects_zh.append(parsed["labels"]["zh"]["intermediate_subject"])
                used_subjects_zh.append(parsed["labels"]["zh"]["final_subject"])
                used_verbs_zh.append(parsed["labels"]["zh"]["cause_verb"])
                used_verbs_zh.append(parsed["labels"]["zh"]["intermediate_verb"])
                used_verbs_zh.append(parsed["labels"]["zh"]["final_verb"])

            else:
                print(f"âŒ Token split detected at trial {tries}, skipped.")

        except Exception as e:
            print(f"âš  Error at trial {tries}: {e}")
            time.sleep(3)
    
    # ä¿å­˜å½“å‰æ•°æ®ï¼ˆæ— è®ºæ˜¯å¦æ»¡50æ¡ï¼‰
    output_file = output_dir / f"{domain}.jsonl"
    print(f"\nSaving {len(clean_samples)} samples to {output_file}")
    with open(output_file, "w", encoding="utf-8") as f:
        for item in clean_samples:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
    if output_file.exists():
        with open(output_file, "r", encoding="utf-8") as f:
            saved_lines = f.readlines()
        print(f"Verified: File contains {len(saved_lines)} lines")
    else:
        print("âŒ Error: File was not created!")
    
    # æ‰“å°æ—¥å¿—
    if len(clean_samples) < n_target:
        print(f"ğŸš¨ [STOPPED EARLY] {domain}: Only collected {len(clean_samples)} samples after {tries} trials.")
    else:
        print(f"ğŸ‰ [DONE] {domain}: Collected {n_target} samples in {tries} trials.")

# æ‰§è¡Œæ‰€æœ‰é¢†åŸŸ
for zh_domain in zh_domains:
    generate_clean_data(domain_mapping[zh_domain])
