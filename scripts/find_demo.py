from transformers import AutoTokenizer

# åŠ è½½ Qwen tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-1.8B-Chat")

def find_token_indices_for_word(prompt: str, target_word: str):
    # ç¼–ç ï¼Œå¹¶è¿”å› offset æ˜ å°„ï¼ˆå­—ç¬¦çº§åˆ«ä½ç½®ï¼‰
    encoding = tokenizer(prompt, return_offsets_mapping=True, return_tensors="pt", add_special_tokens=False)
    tokens = tokenizer.convert_ids_to_tokens(encoding.input_ids[0])
    offsets = encoding.offset_mapping[0].tolist()

    # æ‰¾åˆ°ç›®æ ‡è¯çš„èµ·æ­¢å­—ç¬¦ä½ç½®
    start_idx = prompt.index(target_word)
    end_idx = start_idx + len(target_word)

    # é€‰å‡ºä¸ç›®æ ‡è¯æœ‰é‡å çš„ token
    target_token_indices = [
        i for i, (s, e) in enumerate(offsets)
        if not (e <= start_idx or s >= end_idx)
    ]

    print(f"\nğŸ” Prompt: {prompt}")
    print(f"ğŸ¯ Target word: '{target_word}'")
    print(f"ğŸ§© Token indices: {target_token_indices}")
    print(f"ğŸ§· Tokens: {[tokens[i] for i in target_token_indices]}")

    return target_token_indices, [tokens[i] for i in target_token_indices]

# âœ… DEMO ç¤ºä¾‹
prompt = "Once vaccine develops, immunity strengthdhucfhens, then antibodies reduce."
target_word = "strengthdhucfhens"

find_token_indices_for_word(prompt, target_word)